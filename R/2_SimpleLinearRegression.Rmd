---
title: "Simple Linear Regression"
output: html_notebook
---

Course: https://edstem.org/au/courses/7175/lessons/16373/slides/126443
Useful videos: https://www.youtube.com/watch?v=PaFPbb66DxQ&list=PLblh5JKOoLUIzaEkCLIUxQFjPIlapw8nU

The MASS R library contains the Boston dataset, which records medv (median house value) for 506506 neighbourhoods around Boston. Fit a simple linear regression model using medv as your response variable and lstat (per cent of households with low socioeconomic status) as your predictor variable. 

We will perform the simple linear regression analysis in the following steps:

Step 1: Inspect, summarise and visualise your dataset.

Step 2: Produce a scatter diagram of the response variable versus the explanatory variable. What is the relationship between these two variables?

Step 3: Fit the SLR model using the lm() function in R. Write down the resulting regression equation. What does this equation tell you?

Step 4: Assess the accuracy of the coefficient estimates using the R output.

Step 5: Assess the accuracy of the SLR model.

Step 6: Identify any potential problems in your analysis by using diagnostic plots.

Step 7: Use the regression equation to predict the value of medv for lstat values of 55, 1010 and 1515.

---------

## Step 1:
The following code loads the library and gives the names of the columns.

```{r}
library(MASS)
names(Boston)
```

The "dim()" function tells us how many rows and columns are in the dataset.

```{r}
dim(Boston)
```

The "summary()" fucntion produces a numerical summary for each variable in our dataset.

```{r}
summary(Boston$medv)
```

```{r}
summary(cbind(Boston$medv, Boston$lstat))
```

------------

## Step 2:

Comparative boxplots are a convenient way of graphically depicting groups of numerical data.

```{r}
boxplot(cbind(Boston$medv, Boston$lstat), horizontal=TRUE)
```

The boxplot suggests there might be some outliers affecting our analysis.

The scatterplot below is used to assess the relationship between the variables of interest.

```{r}
plot(Boston$lstat, Boston$medv)
```

----------

## Step 3:
We now look to fit a linear model of the form

$$
Y_i = \beta_0+\beta_1 X_i + \epsilon_i
$$

There are two important assumptions in the SLR analysis:
* $X_i$ are measured without error, so $x_i$ are fixed constants;
* The errors $\epsilon_i$ are independent from each other and are normally distributed with a mean of 0 and a common variance $\sigma^2$, that is $e_i \sim N(0, \sigma^2)$.

The least squares principle finds beta coefficients that minimise the sum of squares of the residuals.
$$
RSS = \sum_{i=1}^{n}(y_i - (\hat\beta_0+\hat\beta_1 x_i))^2
$$

```{r}
lm.fit <- lm(Boston$medv~Boston$lstat)
summary(lm.fit)
```

Thus
$$
medv = 34.55-0.95*lstat+\epsilon
$$

Which says that the 'medv' (median house value) decreases as 'lstat' (percent of households with low socioeconomic status) increases.

We can graph the fitted line over the scatterplot.

```{r}
plot(Boston$lstat, Boston$medv)
abline(lm.fit,col='red',lwd=3)
```

----------

## Step 4:
Next, we assess the beta values generated by the linear model. This is done by calculating the standard error of each term, using the residual standard error as a stepping stone. The estimated standard error terms can be written as follows

$$
\widehat{SE}(\hat\beta_0) \quad \widehat{SE}(\hat\beta_1)
$$

The summary data of the line fit we calculated earlier has these values, 0.5626 and 0.0387 respectively.

Standard errors can be used to compute a $(1 - \alpha)100\%$ confidence interval for the beta figures.
The "confint()" function will calculate a Student-t confidence score for us. The 95% confidence interval is used by default.

```{r}
confint(lm.fit)
```

Which tells us that the 95% confidence interval for the slope, $\beta_1$, is [33.448, 35.659], and that the 95% confidence interval for the intercept, $\beta_0$, is [-1.026, -0.874].

Standard errors can also be used to perform hypothesis testing on the coefficients. The most common test is
$$
H_0: \beta_1 = 0 \\
H_1: \beta_1 \neq 0
$$

The summary data previously calculated provides t-statistic and p values in the penultimate and ultimate columns (-24.53 and <2e-16 respectively). With these statistics, we reject the null hypothesis.

------

## Step 5:
Now we asses the accuracy of the SLR model. The quality of the linear regression fit is typically assessed using the residual standard error (RSE) and the $R^2$ statistic.

The RSE is an estimate of the standard deviation of the epsilon term. The RSE is considered a measure of 'lack of fit'. The RSE will be small for the model which fits the data well. In our example, the RSE is 6.216 (which is small).

$R^2$ measures the contribution of the independent variable(s) in the model, and is known as the coefficient of determination. $R^2$ is rather low in our example (0.5441). This statistic can be read as "54% of the variation in the 'medv' data can be explained by the 'lstat' variable." More complex modelling or removing outliers might be needed to improve $R^2$.

-------

## Step 6:

Some potential problems may arise in linear regression. Below we list such possible issues and suggest some diagnostic plots that can be used to identify them.

1. Non-linearity of the response-predictor relationship: \
Residual plots - we plot residuals $\epsilon_i = y_i-\hat{y}$ versus $x_i$ or versus fitted values $\hat{y}_i$. Non-linearity can be seen in the presence of a pattern, such as a 'U' shape.

2. Correlation of error terms: \
If there is a  time component in the data, we plot the residuals as a function of time (when data is time dependent).

3. Non-constant variance of error terms: \
Residual plots - heteroscedasticity can be seen in the form of a funnel shape in the residuals versus fitted values plot.

4. Outliers: \
Outliers are observations for which the response $y_i$ is unusually far from the predicted value. A plot of studentized residuals, computed by dividing each residual by its estimated standard error (RSE). Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.

5. High-leverage points: \
Observations with high-leverage have an unusual value for $x_i$. Plot of studentized residuals versus the leverage statistic defined by
$$
h_i = \frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum_{k=1}^{n}(x_k-\bar{x})^2}
$$

The leverage statistic has values between 1/n and 1, with average 2/n. If given observation has $h_i$ that exceeds 2 or 3 times the average 2/n, then we may suspect the corresponding point has high leverage.

6. Collinearity: \
Collinearity refers to the situation in which two or more predictor variables are closely related to one another (not the case in SLR).

Looking at the above scatterplot with fitted line, there is some evidence of non-linearity to explore.

```{r}
plot(lm.fit)
```

The above plots confirm some evidence of non-linearity. The 'leverage' statistics previously alluded to can be computed with the "hatvalues()" function. The red line in the following plot represents the $3*2/n$ level. (Note, this line looks slightly higher than the one painted in the course notes).

```{r}
plot(hatvalues(lm.fit))
lines(rep(3*2/506,506),col=2)
```

To identify the index of the largest element of a vector of leverage statistics we use the function "which.max()"

```{r}
which.max(hatvalues(lm.fit))
```

-----

## Step 7:

Let us now use the "predict()" function to calculate the predicted value of medv for lstat equal to 5, 10, and 15.

```{r}
lm.fit<-lm(medv~lstat,data=Boston) 
predict(lm.fit, data.frame(lstat=c(5,10,15)))
```


The "predict()" function can also be used to produce confidence intervals and prediction intervals for the predicted value of medv for lstat equal to 5, 10, 15.

```{r}
lm.fit<-lm(medv~lstat,data=Boston) 
predict(lm.fit, data.frame(lstat=c(5,10,15)))
predict(lm.fit, data.frame(lstat=c(5,10,15)),interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),interval="prediction")
```

As above, the 95% confidence interval associated with a lstat value of 10 is [24.47, 25.63], and the 95% prediction interval is [12.83, 37.28].