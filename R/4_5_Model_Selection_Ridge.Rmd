---
title: "Model Selection: Ridge Regression"
output: html_notebook
---

## Overview
Wiki: "Ridge regression is a method of estimating the coefficients of
multiple-regression models in scenarios where linearly independent
variables are highly correlated."

Ridge regression increases some bias in the model, but intends to
reduce variance. It penalises large coefficient values; the ridge
model 'pulls back' on the coefficients.

## Task
Analyse the properties of ridge regression through the ISLR package
dataset 'credit'.

1. Load the dataset.
2. Define a grid for the tuning parameter $\lambda$, then perform
the ridge regression.
3. Check the assertion that ridge regression gives similar answers
to ordinary regression when $\lambda$ is small.
4. Check the effect of having a large $\lambda$ value.
5. Split the data equally into training and test data, and check the
assertion that MSE is improved under the right $\lambda$.
6. Use 5-fold cross-validation to select the optimal tuning value.
7. Check the minimum $\lambda$ given how close it is to the
boundary.

## Solution

# 1.
Load the dataset.

```{r}
library(ISLR)
data('Credit')
attach(Credit)

Credit = Credit[,-1]
head(Credit)
```

Set up the variables.

```{r}
Y = Credit$Balance
X = model.matrix(Balance~., data=Credit)[,-1]
head(X)
```

# 2.
Define a grid for the tuning parameter $\lambda$, then perform
the ridge regression.

```{r}
library(glmnet)

grid = 10^seq(5, -2, length=100) # Sets up a n=100 vector of values between 100000 and 0.01

ridge.mod = glmnet(X, Y, alpha=0, lambda=grid) # This is the ridge regression

plot( # The plot shows the regression coefs for the Income predictor as a function of lambda
  seq(5, -2, length=100),
  coef(ridge.mod)[2,],
  type='l',
  ylab='Income Coefficient',
  xlab=expression(log(lambda)/log(10))
  )
```

# 3.
Check the assertion that ridge regression gives similar answers when $\lambda$ is small.

```{r}
lm.mod = lm(Y~X)
round(cbind(lm.mod$coefficients, coef(ridge.mod)[,100]), 3)
```

# 4.
Check the effect of having a large $\lambda$ value.
We can see from the output below that the coefficients have been aggressively restrained.

```{r}
round(cbind(lm.mod$coefficients, coef(ridge.mod)[,1]), 3)
```

# 5.
Split the data equally into training and test data, and check the
assertion that MSE is improved under the right $\lambda$.

First we check the MSE under the normal regression model.

```{r}
set.seed(1)
train = sample(1:nrow(X), nrow(X)/2) # Index numbers for half of the data set
test = -train # The negative of the initial index numbers

linear.mod = lm(Y[train]~X[train,]) # Set up the model

# Manual calculation of the test prediction based on lm coefficients
linear.pred = coef(linear.mod)[1] + X[test,] %*% coef(linear.mod)[-1]

# MSE of prediction for test data under normal model
mean((linear.pred - Y[test])^2) 
```

Now we check the MSE under the ridge regression model for $\lambda$
values 0.01, 7, and 20.

```{r}
ridge.mod = glmnet(X[train,], Y[train], alpha=0, lambda=grid, thresh=1e-12)

ridge.pred1 = predict(ridge.mod, s=0.01, newx=X[test,])
ridge.pred2 = predict(ridge.mod, s=7, newx=X[test,])
ridge.pred3 = predict(ridge.mod, s=20, newx=X[test,])

mean((ridge.pred1 - Y[test])^2)
mean((ridge.pred2 - Y[test])^2)
mean((ridge.pred3 - Y[test])^2)

```

The MSE has reduced reasonably for $\lambda$ value of 7.

# 6.
In general is better to use cross-validation than random lambda selections.
Use 5-fold cross-validation to select the optimal tuning value based on MSE.

```{r}
set.seed(2)

cv.out = cv.glmnet(X[train,], Y[train], alpha=0, folds=5)
plot(cv.out) # Shows the 5-fold CV model MSE given log(lambda) choices.
```

```{r}
bestlam = cv.out$lambda.min
bestlam
log(bestlam)

ridge.pred = predict(ridge.mod, s=bestlam, newx=X[test,])
mean((ridge.pred - Y[test])^2) # Test MSE given
```

# 7
Check the minimum $\lambda$ given how close it is to the
boundary.

```{r}
set.seed(2)

cv.out = cv.glmnet(X[train,], Y[train], alpha=0, lambda=grid, nfolds=5)
plot(cv.out)
```

```{r}
bestlam = cv.out$lambda.min
bestlam

ridge.pred = predict(ridge.mod, s=bestlam, newx=X[test,])
mean((ridge.pred - Y[test])^2)
```

"We can now see a clear improvement compared to the ordinary regression.
For $\lambda$ equal to 0.14 we have MSE 10571.51. MSE of the ordinary
regression was 10691.12. Using a fine grid search for lambda coud lead to
better results since for lambda equal to 7 we get MSE = 10497.72."