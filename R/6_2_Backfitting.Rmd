---
title: "Backfitting"
output: html_notebook
---

Wiki: "In statistics, the backfitting algorithm is a simple
iteraive procedure used to fit a generalised additive model."
https://en.wikipedia.org/wiki/Backfitting_algorithm

# Activity
Suppose that we want to do multiple linear regression, but
don't have the software to do it. Instead we use the
backfitting method to keep all but one coefficient fixed and update
the one coefficient with the linear regression method.
The process is continued until 'convergence' to the true value.
It's not easy to know when convergence will occur, but we can
iterate the process as many times as we like.

1) Generate a response Y and two predictors X1 and X2 with
n=100.
2) Initialise Beta1 to take on a value of your choice.
3) Keeping Beta1 fixed, fit the model
$$ Y - \hat\beta_1 X_1 = \beta_0 + \beta_2 X_2 + \epsilon$$
4) Keeping Beta2 fixed, fit the model
$$ Y - \hat\beta_2 X_2 = \beta_0 + \beta_1 X_1 + \epsilon$$
5) Write a loop to repeat (2) and (3) 10 times.
Report estimates for betas at each iteration to the
console and as a plot.
6) Compare answer in (4) to simply running a multiple
linear regression to predict Y using X1 and X2. Use the
abline() function to overlay the multiple linear regression
coefficient estimates on the plot obtained in (4).

# 1
```{r}
set.seed(0)

n = 100
x1 = rnorm(n)
x2 = rnorm(n)

# The 'true' values of beta_i:
beta_0 = 3
beta_1 = 5
beta_2 = -0.2

# Predetermined response variable
y = beta_0 + beta_1*x1 + beta_2*x2 + 0.1*rnorm(n)
```

# 2
```{r}
beta_1_hat = -3 # Any old starting point for b1
```

# 3-5
```{r}
iterations = 10

beta_0_estimates = c()
beta_1_estimates = c()
beta_2_estimates = c()

for(i in 1:iterations){
  a = y - beta_1_hat*x1
  beta_2_hat = lm(a ~ x2)$coef[2]
  
  a = y - beta_2_hat*x2
  m = lm(a ~ x1)
  
  beta_1_hat = m$coef[2]
  beta_0_hat = m$coef[1]
  
  beta_0_estimates = c(beta_0_estimates, beta_0_hat)
  beta_1_estimates = c(beta_1_estimates, beta_1_hat)
  beta_2_estimates = c(beta_2_estimates, beta_2_hat)
}
```

# 6
The iterated coefficient estimates are plotted as black dots,
the true values of the betas are in green, and the
estimates from the multiple linear regression are in grey.

We see that the backfitting iterations converge to the same
values as the solution of the multiple regression fit.
In the case of beta_2, the multiple regression value converges
perfectly (or at least to 2 decimal places), so the line is
sitting behind the gray line of the multiple regression.
```{r}
m = lm(y ~ x1 + x2)

old_par = par(mfrow=c(1,3))

plot(1:iterations, beta_0_estimates, main='beta_0', pch=19,
     ylim=c(beta_0*0.999, max(beta_0_estimates)))
abline(h=beta_0, col='green', lwd=4)
abline(h=m$coefficients[1], col='gray', lwd=4)
grid()

plot(1:iterations, beta_1_estimates, main='beta_1', pch=19)
abline(h=beta_1, col='green', lwd=4)
abline(h=m$coefficients[2], col='gray', lwd=4)
grid()

beta_2
plot(1:iterations, beta_2_estimates, main='beta_2', pch=19)
abline(h=beta_2, col='green', lwd=5)
abline(h=m$coefficients[3], col='gray', lwd=4)
grid()
```

