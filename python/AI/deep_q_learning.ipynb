{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Reinforcement Learning (DQN) Tutorial\n",
                "=====================================\n",
                "In this exercise, you will practice how to use PyTorch to train a Deep Q-learning (DQN) agent\n",
                "on the CartPole-v0 task from OpenAI Gym. Specifically, you will need to implement several functions/classes which are necessary components of the DQN algorithm:\n",
                "\n",
                "1.  ``ReplayMemory``\n",
                "2.  ``Q-Network``\n",
                "3.  ``Optimize_Model``\n",
                "4.  ``Select_action``\n",
                "\n",
                "Each function/class has its own cell where you can see more details, please complete the exercices marked with TODO.\n",
                "\n",
                "**Packages**\n",
                "\n",
                "\n",
                "We need OpenAI gym for the environment (Install using `pip install gym`).\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gym\n",
                "from gym import wrappers\n",
                "import random\n",
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.autograd import Variable\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import pdb\n",
                "\n",
                "# if gpu is to be used\n",
                "use_cuda = torch.cuda.is_available()\n",
                "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
                "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
                "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
                "Tensor = FloatTensor\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hyperparameters\n",
                "\n",
                "After implementing the neural network model and other necessary functions, you can try to do more hyperparameters tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# hyper parameters\n",
                "EPISODES = 400  # number of episodes\n",
                "EPS_START = 0.9  # e-greedy threshold start value\n",
                "EPS_END = 0.05  # e-greedy threshold end value\n",
                "EPS_DECAY = 200  # e-greedy threshold decay\n",
                "GAMMA = 0.99  # Q-learning discount factor\n",
                "LR = 0.01  # NN optimizer learning rate\n",
                "HIDDEN_LAYER = 64  # NN hidden layer size\n",
                "BATCH_SIZE = 64  # Q-learning batch size"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Environment \n",
                "CartPole-v0 is a classic reinforcement learning environment from OpenAI Gym. In this environment, the agent has to decide between two actions $-$ moving the cart left or right $-$ so that the pole attached to it stays upright. \n",
                "\n",
                "As the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action. In this task, rewards are +1 for every incremental timestep and the environment terminates if the pole falls over too far or the cart moves more then 2.4 units away from center. This means better performing scenarios will run for longer duration, accumulating larger return. The CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.). \n",
                "\n",
                "We first set up the envrionment of CartPole-v0 using Gym."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = gym.make('CartPole-v0')\n",
                "env._max_episode_steps = 500\n",
                "env = wrappers.Monitor(env, './tmp/cartpole-v0-1', force=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Replay Memory\n",
                "-------------\n",
                "\n",
                "We will be using experience replay memory for training our DQN. It stores\n",
                "the transitions that the agent observes, allowing us to reuse this data\n",
                "later. By sampling from it randomly, the transitions that build up a\n",
                "batch are decorrelated. It has been shown that this greatly stabilizes\n",
                "and improves the DQN training procedure.\n",
                "\n",
                "For this, we're going to implement the replay memory buffer as a python class:\n",
                "\n",
                "-  ``ReplayMemory`` $-$ a cyclic buffer of bounded size that holds the\n",
                "   transitions observed recently. It also implements a ``.sample()``\n",
                "   method for selecting a random batch of transitions for training, and a ``.push()`` method for adding a new transition while potentially remove the oldest saved transition if the size of memory buffer exceeds the capacity. Each tranisiton is a tuple which consists of state, action, next_state, reward.\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReplayMemory:\n",
                "    def __init__(self, capacity):\n",
                "        self.capacity = capacity\n",
                "        self.memory = []\n",
                "\n",
                "    def push(self, transition):\n",
                "        #TODO\n",
                "        return\n",
                "    def sample(self, batch_size):\n",
                "        #TODO\n",
                "        return\n",
                "    def __len__(self):\n",
                "        return len(self.memory)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q-Network\n",
                "Next, we need to define our model. Our model will consist of fully connected layers that takes in the\n",
                "state returned by the envrionment. It has two\n",
                "outputs, representing $Q(s, \\mathrm{left})$ and\n",
                "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
                "network). In effect, the network is trying to predict the *expected return* of\n",
                "taking each action given the current input.\n",
                "\n",
                "Define a 2-layer fully connected neural network with ${\\rm tanh}$ activation at the hidden layer, followed by the output layer. The hidden layer size is decided by the hyperparameter 'HIDDEN_LAYER' and the size of the output is 2. You could also try any other architectures you want."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Network(nn.Module):\n",
                "    def __init__(self):\n",
                "        nn.Module.__init__(self)\n",
                "        # TODO\n",
                "        return \n",
                "    def forward(self, x):\n",
                "        # TODO\n",
                "        return"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create the model, memory buffer and optimizer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'Network' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m\u003cipython-input-1-77061e528f9c\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'Network' is not defined"
                    ]
                }
            ],
            "source": [
                "model = Network()\n",
                "if use_cuda:\n",
                "    model.cuda()\n",
                "memory = ReplayMemory(10000)\n",
                "optimizer = optim.Adam(model.parameters(), LR)\n",
                "steps_done = 0\n",
                "episode_durations = []"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "DQN algorithm\n",
                "-------------\n",
                "\n",
                "Our environment is deterministic, so all equations presented here are\n",
                "also formulated deterministically for the sake of simplicity. In the\n",
                "reinforcement learning literature, they would also contain expectations\n",
                "over stochastic transitions in the environment.\n",
                "\n",
                "Our aim will be to train a policy that tries to maximize the discounted,\n",
                "cumulative reward\n",
                "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
                "$R_{t_0}$ is also known as the *return*. The discount,\n",
                "$\\gamma$, should be a constant between $0$ and $1$\n",
                "that ensures the sum converges. It makes rewards from the uncertain far\n",
                "future less important for our agent than the ones in the near future\n",
                "that it can be fairly confident about.\n",
                "\n",
                "The main idea behind Q-learning is that if we had a function\n",
                "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
                "us what our return would be, if we were to take an action in a given\n",
                "state, then we could easily construct a policy that maximizes our\n",
                "rewards:\n",
                "\n",
                "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
                "\n",
                "However, we don't know everything about the world, so we don't have\n",
                "access to $Q^*$. But, since neural networks are universal function\n",
                "approximators, we can simply create one and train it to resemble\n",
                "$Q^*$.\n",
                "\n",
                "For our training update rule, we'll use a fact that every $Q$\n",
                "function for some policy obeys the Bellman equation:\n",
                "\n",
                "$$Q^{\\pi}(s, a) = r + \\gamma\\,Q^{\\pi}(s', \\pi(s'))$$\n",
                "\n",
                "The difference between the two sides of the equality is known as the\n",
                "temporal difference error, $\\delta$:\n",
                "\n",
                "$$\\delta = Q(s, a) - (r + \\gamma \\max_b Q(s', b))$$\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Training\n",
                "--------\n",
                "\n",
                "First, we need to implement some utility functions for our training procedure\n",
                "\n",
                "-  ``select_action`` $-$ will select an action accordingly to an epsilon\n",
                "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
                "   the action, and sometimes we'll just sample one uniformly. The\n",
                "   probability of choosing a random action will start at ``EPS_START``\n",
                "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
                "   controls the rate of the decay.\n",
                "   \n",
                "-  ``optimize_model`` $-$ performs a single step of the optimization. It first samples a batch, concatenates \n",
                "all the tensors into a single one, then we'll use the model to calculate the Q values for different state and use bellman euqation to optmize our model.\n",
                "   \n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def select_action(state):\n",
                "    global steps_done\n",
                "    sample = random.random()\n",
                "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
                "    steps_done += 1\n",
                "    \n",
                "    # TODO: implement the epsilon greddy policy, i.e., if the sampled number is bigger \n",
                "    # than the threshold, then we use the model for action selection, otherwise \n",
                "    # we'll randomly sample one of the action.\n",
                "    return"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def optimize_model():\n",
                "    if len(memory) \u003c BATCH_SIZE:\n",
                "        return\n",
                "\n",
                "    # random transition batch is taken from experience replay memory\n",
                "    transitions = memory.sample(BATCH_SIZE)\n",
                "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
                "\n",
                "    batch_state = Variable(torch.cat(batch_state))\n",
                "    batch_action = Variable(torch.cat(batch_action))\n",
                "    batch_reward = Variable(torch.cat(batch_reward))\n",
                "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
                "\n",
                "\n",
                "    # TODO: Calculate current Q values estimated by NN for all actions\n",
                "    current_q_values = None\n",
                "    \n",
                "    # TODO: Calculate Q values for the next state of actions which gives maximum Q value\n",
                "    max_next_q_values = None\n",
                "    \n",
                "    # TODO: Calculate the target Q value (r + max Q_{next_state})\n",
                "    expected_q_values = None\n",
                "    \n",
                "    # loss is measured from error between current and newly expected Q values\n",
                "    loss = F.smooth_l1_loss(current_q_values.squeeze(), expected_q_values)\n",
                "\n",
                "    # backpropagation of loss to NN\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below, you can find the main training loop. At the beginning we reset\n",
                "the environment and initialize the ``state`` Tensor. Then, we sample\n",
                "an action, execute it, observe the next state and the reward (always\n",
                "1), and optimize our model once. When the episode ends (our model\n",
                "fails), we restart the loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 0 finished after 29 steps\n\u001b[99m Episode 1 finished after 22 steps\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 2 finished after 29 steps\n\u001b[99m Episode 3 finished after 18 steps\n\u001b[99m Episode 4 finished after 11 steps\n\u001b[99m Episode 5 finished after 16 steps\n\u001b[99m Episode 6 finished after 16 steps\n\u001b[99m Episode 7 finished after 23 steps\n\u001b[99m Episode 8 finished after 16 steps\n\u001b[99m Episode 9 finished after 19 steps\n\u001b[99m Episode 10 finished after 11 steps\n\u001b[99m Episode 11 finished after 12 steps\n\u001b[99m Episode 12 finished after 13 steps\n\u001b[99m Episode 13 finished after 10 steps\n\u001b[99m Episode 14 finished after 11 steps\n\u001b[99m Episode 15 finished after 12 steps\n\u001b[99m Episode 16 finished after 11 steps\n\u001b[99m Episode 17 finished after 8 steps\n\u001b[99m Episode 18 finished after 11 steps\n\u001b[99m Episode 19 finished after 9 steps\n\u001b[99m Episode 20 finished after 11 steps\n\u001b[99m Episode 21 finished after 10 steps\n\u001b[99m Episode 22 finished after 9 steps\n\u001b[99m Episode 23 finished after 9 steps\n\u001b[99m Episode 24 finished after 10 steps\n\u001b[99m Episode 25 finished after 11 steps\n\u001b[99m Episode 26 finished after 10 steps\n\u001b[99m Episode 27 finished after 9 steps\n\u001b[99m Episode 28 finished after 11 steps\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 29 finished after 14 steps\n\u001b[99m Episode 30 finished after 9 steps\n\u001b[99m Episode 31 finished after 16 steps\n\u001b[99m Episode 32 finished after 11 steps\n\u001b[99m Episode 33 finished after 11 steps\n\u001b[99m Episode 34 finished after 10 steps\n\u001b[99m Episode 35 finished after 9 steps\n\u001b[99m Episode 36 finished after 10 steps\n\u001b[99m Episode 37 finished after 9 steps\n\u001b[99m Episode 38 finished after 9 steps\n\u001b[99m Episode 39 finished after 11 steps\n\u001b[99m Episode 40 finished after 12 steps\n\u001b[99m Episode 41 finished after 11 steps\n\u001b[99m Episode 42 finished after 12 steps\n\u001b[99m Episode 43 finished after 9 steps\n\u001b[99m Episode 44 finished after 11 steps\n\u001b[99m Episode 45 finished after 11 steps\n\u001b[99m Episode 46 finished after 9 steps\n\u001b[99m Episode 47 finished after 10 steps\n\u001b[99m Episode 48 finished after 10 steps\n\u001b[99m Episode 49 finished after 12 steps\n\u001b[99m Episode 50 finished after 11 steps\n\u001b[99m Episode 51 finished after 9 steps\n\u001b[99m Episode 52 finished after 9 steps\n\u001b[99m Episode 53 finished after 11 steps\n\u001b[99m Episode 54 finished after 9 steps\n\u001b[99m Episode 55 finished after 10 steps\n\u001b[99m Episode 56 finished after 10 steps\n\u001b[99m Episode 57 finished after 23 steps\n\u001b[99m Episode 58 finished after 20 steps\n\u001b[99m Episode 59 finished after 9 steps\n\u001b[99m Episode 60 finished after 19 steps\n\u001b[99m Episode 61 finished after 10 steps\n\u001b[99m Episode 62 finished after 22 steps\n\u001b[99m Episode 63 finished after 18 steps\n\u001b[99m Episode 64 finished after 9 steps\n\u001b[99m Episode 65 finished after 14 steps\n\u001b[99m Episode 66 finished after 21 steps\n\u001b[99m Episode 67 finished after 9 steps\n\u001b[99m Episode 68 finished after 21 steps\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 69 finished after 9 steps\n\u001b[99m Episode 70 finished after 20 steps\n\u001b[99m Episode 71 finished after 19 steps\n\u001b[99m Episode 72 finished after 27 steps\n\u001b[99m Episode 73 finished after 23 steps\n\u001b[99m Episode 74 finished after 21 steps\n\u001b[99m Episode 75 finished after 29 steps\n\u001b[99m Episode 76 finished after 13 steps\n\u001b[99m Episode 77 finished after 17 steps\n\u001b[99m Episode 78 finished after 10 steps\n\u001b[99m Episode 79 finished after 12 steps\n\u001b[99m Episode 80 finished after 13 steps\n\u001b[99m Episode 81 finished after 9 steps\n\u001b[99m Episode 82 finished after 41 steps\n\u001b[99m Episode 83 finished after 17 steps\n\u001b[99m Episode 84 finished after 9 steps\n\u001b[99m Episode 85 finished after 21 steps\n\u001b[99m Episode 86 finished after 19 steps\n\u001b[99m Episode 87 finished after 12 steps\n\u001b[99m Episode 88 finished after 18 steps\n\u001b[99m Episode 89 finished after 10 steps\n\u001b[99m Episode 90 finished after 16 steps\n\u001b[99m Episode 91 finished after 22 steps\n\u001b[99m Episode 92 finished after 43 steps\n\u001b[99m Episode 93 finished after 11 steps\n\u001b[99m Episode 94 finished after 33 steps\n\u001b[99m Episode 95 finished after 52 steps\n\u001b[99m Episode 96 finished after 44 steps\n\u001b[99m Episode 97 finished after 33 steps\n\u001b[99m Episode 98 finished after 31 steps\n\u001b[99m Episode 99 finished after 10 steps\n\u001b[99m Episode 100 finished after 19 steps\n\u001b[99m Episode 101 finished after 18 steps\n\u001b[99m Episode 102 finished after 41 steps\n\u001b[99m Episode 103 finished after 9 steps\n\u001b[99m Episode 104 finished after 15 steps\n\u001b[99m Episode 105 finished after 20 steps\n\u001b[99m Episode 106 finished after 19 steps\n\u001b[99m Episode 107 finished after 12 steps\n\u001b[99m Episode 108 finished after 59 steps\n\u001b[99m Episode 109 finished after 10 steps\n\u001b[99m Episode 110 finished after 18 steps\n\u001b[99m Episode 111 finished after 19 steps\n\u001b[99m Episode 112 finished after 36 steps\n\u001b[99m Episode 113 finished after 31 steps\n\u001b[99m Episode 114 finished after 45 steps\n\u001b[99m Episode 115 finished after 50 steps\n\u001b[99m Episode 116 finished after 29 steps\n\u001b[99m Episode 117 finished after 35 steps\n\u001b[99m Episode 118 finished after 20 steps\n\u001b[99m Episode 119 finished after 9 steps\n\u001b[99m Episode 120 finished after 26 steps\n\u001b[99m Episode 121 finished after 15 steps\n\u001b[99m Episode 122 finished after 43 steps\n\u001b[99m Episode 123 finished after 13 steps\n\u001b[99m Episode 124 finished after 9 steps\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 125 finished after 42 steps\n\u001b[99m Episode 126 finished after 16 steps\n\u001b[99m Episode 127 finished after 9 steps\n\u001b[99m Episode 128 finished after 17 steps\n\u001b[99m Episode 129 finished after 12 steps\n\u001b[99m Episode 130 finished after 14 steps\n\u001b[99m Episode 131 finished after 17 steps\n\u001b[99m Episode 132 finished after 28 steps\n\u001b[99m Episode 133 finished after 38 steps\n\u001b[99m Episode 134 finished after 30 steps\n\u001b[99m Episode 135 finished after 42 steps\n\u001b[99m Episode 136 finished after 21 steps\n\u001b[99m Episode 137 finished after 17 steps\n\u001b[99m Episode 138 finished after 12 steps\n\u001b[99m Episode 139 finished after 13 steps\n\u001b[99m Episode 140 finished after 42 steps\n\u001b[99m Episode 141 finished after 32 steps\n\u001b[99m Episode 142 finished after 44 steps\n\u001b[99m Episode 143 finished after 11 steps\n\u001b[99m Episode 144 finished after 12 steps\n\u001b[99m Episode 145 finished after 56 steps\n\u001b[99m Episode 146 finished after 11 steps\n\u001b[99m Episode 147 finished after 9 steps\n\u001b[99m Episode 148 finished after 46 steps\n\u001b[99m Episode 149 finished after 11 steps\n\u001b[99m Episode 150 finished after 11 steps\n\u001b[99m Episode 151 finished after 14 steps\n\u001b[99m Episode 152 finished after 26 steps\n\u001b[99m Episode 153 finished after 31 steps\n\u001b[99m Episode 154 finished after 10 steps\n\u001b[99m Episode 155 finished after 32 steps\n\u001b[99m Episode 156 finished after 23 steps\n\u001b[99m Episode 157 finished after 11 steps\n\u001b[99m Episode 158 finished after 13 steps\n\u001b[99m Episode 159 finished after 47 steps\n\u001b[99m Episode 160 finished after 45 steps\n\u001b[99m Episode 161 finished after 11 steps\n\u001b[99m Episode 162 finished after 29 steps\n\u001b[99m Episode 163 finished after 18 steps\n\u001b[99m Episode 164 finished after 8 steps\n\u001b[99m Episode 165 finished after 38 steps\n\u001b[99m Episode 166 finished after 14 steps\n\u001b[99m Episode 167 finished after 9 steps\n\u001b[99m Episode 168 finished after 38 steps\n\u001b[99m Episode 169 finished after 37 steps\n\u001b[99m Episode 170 finished after 19 steps\n\u001b[99m Episode 171 finished after 18 steps\n\u001b[99m Episode 172 finished after 12 steps\n\u001b[99m Episode 173 finished after 10 steps\n\u001b[99m Episode 174 finished after 48 steps\n\u001b[99m Episode 175 finished after 36 steps\n\u001b[99m Episode 176 finished after 49 steps\n\u001b[99m Episode 177 finished after 32 steps\n\u001b[99m Episode 178 finished after 19 steps\n\u001b[99m Episode 179 finished after 33 steps\n\u001b[99m Episode 180 finished after 52 steps\n\u001b[99m Episode 181 finished after 16 steps\n\u001b[99m Episode 182 finished after 13 steps\n\u001b[99m Episode 183 finished after 20 steps\n\u001b[99m Episode 184 finished after 36 steps\n\u001b[99m Episode 185 finished after 10 steps\n\u001b[99m Episode 186 finished after 10 steps\n\u001b[99m Episode 187 finished after 11 steps\n\u001b[99m Episode 188 finished after 13 steps\n\u001b[99m Episode 189 finished after 14 steps\n\u001b[99m Episode 190 finished after 12 steps\n\u001b[99m Episode 191 finished after 29 steps\n\u001b[99m Episode 192 finished after 11 steps\n\u001b[99m Episode 193 finished after 12 steps\n\u001b[99m Episode 194 finished after 53 steps\n\u001b[99m Episode 195 finished after 40 steps\n\u001b[99m Episode 196 finished after 10 steps\n\u001b[99m Episode 197 finished after 14 steps\n\u001b[99m Episode 198 finished after 13 steps\n\u001b[99m Episode 199 finished after 18 steps\n\u001b[99m Episode 200 finished after 11 steps\n\u001b[99m Episode 201 finished after 9 steps\n\u001b[99m Episode 202 finished after 29 steps\n\u001b[99m Episode 203 finished after 94 steps\n\u001b[99m Episode 204 finished after 33 steps\n\u001b[99m Episode 205 finished after 14 steps\n\u001b[99m Episode 206 finished after 20 steps\n\u001b[99m Episode 207 finished after 24 steps\n\u001b[99m Episode 208 finished after 35 steps\n\u001b[99m Episode 209 finished after 11 steps\n\u001b[99m Episode 210 finished after 10 steps\n\u001b[99m Episode 211 finished after 45 steps\n\u001b[99m Episode 212 finished after 11 steps\n\u001b[99m Episode 213 finished after 11 steps\n\u001b[99m Episode 214 finished after 44 steps\n\u001b[99m Episode 215 finished after 9 steps\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 216 finished after 8 steps\n\u001b[99m Episode 217 finished after 13 steps\n\u001b[99m Episode 218 finished after 10 steps\n\u001b[99m Episode 219 finished after 57 steps\n\u001b[99m Episode 220 finished after 9 steps\n\u001b[99m Episode 221 finished after 17 steps\n\u001b[99m Episode 222 finished after 20 steps\n\u001b[99m Episode 223 finished after 13 steps\n\u001b[99m Episode 224 finished after 16 steps\n\u001b[99m Episode 225 finished after 28 steps\n\u001b[99m Episode 226 finished after 16 steps\n\u001b[99m Episode 227 finished after 31 steps\n\u001b[99m Episode 228 finished after 23 steps\n\u001b[99m Episode 229 finished after 15 steps\n\u001b[99m Episode 230 finished after 13 steps\n\u001b[99m Episode 231 finished after 24 steps\n\u001b[99m Episode 232 finished after 40 steps\n\u001b[99m Episode 233 finished after 14 steps\n\u001b[99m Episode 234 finished after 17 steps\n\u001b[99m Episode 235 finished after 15 steps\n\u001b[99m Episode 236 finished after 25 steps\n\u001b[99m Episode 237 finished after 34 steps\n\u001b[99m Episode 238 finished after 47 steps\n\u001b[99m Episode 239 finished after 36 steps\n\u001b[99m Episode 240 finished after 15 steps\n\u001b[99m Episode 241 finished after 50 steps\n\u001b[99m Episode 242 finished after 32 steps\n\u001b[99m Episode 243 finished after 44 steps\n\u001b[99m Episode 244 finished after 15 steps\n\u001b[99m Episode 245 finished after 13 steps\n\u001b[99m Episode 246 finished after 35 steps\n\u001b[99m Episode 247 finished after 17 steps\n\u001b[99m Episode 248 finished after 20 steps\n\u001b[99m Episode 249 finished after 19 steps\n\u001b[99m Episode 250 finished after 46 steps\n\u001b[99m Episode 251 finished after 15 steps\n\u001b[99m Episode 252 finished after 56 steps\n\u001b[99m Episode 253 finished after 27 steps\n\u001b[99m Episode 254 finished after 19 steps\n\u001b[99m Episode 255 finished after 47 steps\n\u001b[99m Episode 256 finished after 22 steps\n\u001b[99m Episode 257 finished after 65 steps\n\u001b[99m Episode 258 finished after 15 steps\n\u001b[99m Episode 259 finished after 20 steps\n\u001b[99m Episode 260 finished after 23 steps\n\u001b[99m Episode 261 finished after 19 steps\n\u001b[99m Episode 262 finished after 22 steps\n\u001b[99m Episode 263 finished after 12 steps\n\u001b[99m Episode 264 finished after 11 steps\n\u001b[99m Episode 265 finished after 12 steps\n\u001b[99m Episode 266 finished after 19 steps\n\u001b[99m Episode 267 finished after 19 steps\n\u001b[99m Episode 268 finished after 12 steps\n\u001b[99m Episode 269 finished after 20 steps\n\u001b[99m Episode 270 finished after 12 steps\n\u001b[99m Episode 271 finished after 34 steps\n\u001b[99m Episode 272 finished after 17 steps\n\u001b[99m Episode 273 finished after 26 steps\n\u001b[99m Episode 274 finished after 14 steps\n\u001b[99m Episode 275 finished after 40 steps\n\u001b[99m Episode 276 finished after 17 steps\n\u001b[99m Episode 277 finished after 14 steps\n\u001b[99m Episode 278 finished after 27 steps\n\u001b[99m Episode 279 finished after 33 steps\n\u001b[99m Episode 280 finished after 17 steps\n\u001b[99m Episode 281 finished after 19 steps\n\u001b[99m Episode 282 finished after 11 steps\n\u001b[99m Episode 283 finished after 21 steps\n\u001b[99m Episode 284 finished after 64 steps\n\u001b[99m Episode 285 finished after 29 steps\n\u001b[99m Episode 286 finished after 19 steps\n\u001b[99m Episode 287 finished after 14 steps\n\u001b[99m Episode 288 finished after 17 steps\n\u001b[99m Episode 289 finished after 10 steps\n\u001b[99m Episode 290 finished after 16 steps\n\u001b[99m Episode 291 finished after 22 steps\n\u001b[99m Episode 292 finished after 18 steps\n\u001b[99m Episode 293 finished after 14 steps\n\u001b[99m Episode 294 finished after 12 steps\n\u001b[99m Episode 295 finished after 19 steps\n\u001b[99m Episode 296 finished after 25 steps\n\u001b[99m Episode 297 finished after 56 steps\n\u001b[99m Episode 298 finished after 24 steps\n\u001b[99m Episode 299 finished after 18 steps\n\u001b[99m Episode 300 finished after 27 steps\n\u001b[99m Episode 301 finished after 25 steps\n\u001b[99m Episode 302 finished after 21 steps\n\u001b[99m Episode 303 finished after 38 steps\n\u001b[99m Episode 304 finished after 28 steps\n\u001b[99m Episode 305 finished after 18 steps\n\u001b[99m Episode 306 finished after 11 steps\n\u001b[99m Episode 307 finished after 18 steps\n\u001b[99m Episode 308 finished after 20 steps\n\u001b[99m Episode 309 finished after 13 steps\n\u001b[99m Episode 310 finished after 63 steps\n\u001b[99m Episode 311 finished after 21 steps\n\u001b[99m Episode 312 finished after 10 steps\n\u001b[99m Episode 313 finished after 19 steps\n\u001b[99m Episode 314 finished after 19 steps\n\u001b[99m Episode 315 finished after 12 steps\n\u001b[99m Episode 316 finished after 22 steps\n\u001b[99m Episode 317 finished after 22 steps\n\u001b[99m Episode 318 finished after 31 steps\n\u001b[99m Episode 319 finished after 23 steps\n\u001b[99m Episode 320 finished after 21 steps\n\u001b[99m Episode 321 finished after 14 steps\n\u001b[99m Episode 322 finished after 14 steps\n\u001b[99m Episode 323 finished after 22 steps\n\u001b[99m Episode 324 finished after 23 steps\n\u001b[99m Episode 325 finished after 15 steps\n\u001b[99m Episode 326 finished after 15 steps\n\u001b[99m Episode 327 finished after 14 steps\n\u001b[99m Episode 328 finished after 19 steps\n\u001b[99m Episode 329 finished after 13 steps\n\u001b[99m Episode 330 finished after 10 steps\n\u001b[99m Episode 331 finished after 25 steps\n\u001b[99m Episode 332 finished after 36 steps\n\u001b[99m Episode 333 finished after 11 steps\n\u001b[99m Episode 334 finished after 24 steps\n\u001b[99m Episode 335 finished after 22 steps\n\u001b[99m Episode 336 finished after 10 steps\n\u001b[99m Episode 337 finished after 21 steps\n\u001b[99m Episode 338 finished after 10 steps\n\u001b[99m Episode 339 finished after 22 steps\n\u001b[99m Episode 340 finished after 17 steps\n\u001b[99m Episode 341 finished after 23 steps\n\u001b[99m Episode 342 finished after 14 steps\n\u001b[99m Episode 343 finished after 22 steps\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[99m Episode 344 finished after 14 steps\n\u001b[99m Episode 345 finished after 10 steps\n\u001b[99m Episode 346 finished after 18 steps\n\u001b[99m Episode 347 finished after 28 steps\n\u001b[99m Episode 348 finished after 16 steps\n\u001b[99m Episode 349 finished after 22 steps\n\u001b[99m Episode 350 finished after 31 steps\n\u001b[99m Episode 351 finished after 11 steps\n\u001b[99m Episode 352 finished after 37 steps\n\u001b[99m Episode 353 finished after 12 steps\n\u001b[99m Episode 354 finished after 22 steps\n\u001b[99m Episode 355 finished after 10 steps\n\u001b[99m Episode 356 finished after 14 steps\n\u001b[99m Episode 357 finished after 13 steps\n\u001b[99m Episode 358 finished after 11 steps\n\u001b[99m Episode 359 finished after 25 steps\n\u001b[99m Episode 360 finished after 16 steps\n\u001b[99m Episode 361 finished after 24 steps\n\u001b[99m Episode 362 finished after 16 steps\n\u001b[99m Episode 363 finished after 14 steps\n\u001b[99m Episode 364 finished after 37 steps\n\u001b[99m Episode 365 finished after 29 steps\n\u001b[99m Episode 366 finished after 12 steps\n\u001b[99m Episode 367 finished after 24 steps\n\u001b[99m Episode 368 finished after 31 steps\n\u001b[99m Episode 369 finished after 22 steps\n\u001b[99m Episode 370 finished after 16 steps\n\u001b[99m Episode 371 finished after 18 steps\n\u001b[99m Episode 372 finished after 11 steps\n\u001b[99m Episode 373 finished after 17 steps\n\u001b[99m Episode 374 finished after 15 steps\n\u001b[99m Episode 375 finished after 39 steps\n\u001b[99m Episode 376 finished after 16 steps\n\u001b[99m Episode 377 finished after 11 steps\n\u001b[99m Episode 378 finished after 15 steps\n\u001b[99m Episode 379 finished after 14 steps\n\u001b[99m Episode 380 finished after 15 steps\n\u001b[99m Episode 381 finished after 19 steps\n\u001b[99m Episode 382 finished after 17 steps\n\u001b[99m Episode 383 finished after 18 steps\n\u001b[99m Episode 384 finished after 15 steps\n\u001b[99m Episode 385 finished after 25 steps\n\u001b[99m Episode 386 finished after 13 steps\n\u001b[99m Episode 387 finished after 31 steps\n\u001b[99m Episode 388 finished after 15 steps\n\u001b[99m Episode 389 finished after 18 steps\n\u001b[99m Episode 390 finished after 14 steps\n\u001b[99m Episode 391 finished after 18 steps\n\u001b[99m Episode 392 finished after 40 steps\n\u001b[99m Episode 393 finished after 17 steps\n\u001b[99m Episode 394 finished after 9 steps\n\u001b[99m Episode 395 finished after 10 steps\n\u001b[99m Episode 396 finished after 28 steps\n\u001b[99m Episode 397 finished after 19 steps\n\u001b[99m Episode 398 finished after 27 steps\n\u001b[99m Episode 399 finished after 31 steps\nComplete\n"
                }
            ],
            "source": [
                "for e in range(EPISODES):\n",
                "    state = env.reset()\n",
                "    steps = 0\n",
                "    while True:\n",
                "        env.render()\n",
                "        action = select_action(FloatTensor([state]))\n",
                "\n",
                "        next_state, reward, done, _ = env.step(action[0, 0].item())\n",
                "        # negative reward when attempt ends\n",
                "        if done:\n",
                "            reward = -1\n",
                "\n",
                "        memory.push((FloatTensor([state]),\n",
                "                     action,  # action is already a tensor\n",
                "                     FloatTensor([next_state]),\n",
                "                     FloatTensor([reward])))\n",
                "\n",
                "        optimize_model()\n",
                "\n",
                "        state = next_state\n",
                "        steps += 1\n",
                "\n",
                "        if done:\n",
                "            print(\"{2} Episode {0} finished after {1} steps\"\n",
                "                  .format(e, steps, '\\033[92m' if steps \u003e= 195 else '\\033[99m'))\n",
                "            episode_durations.append(steps)\n",
                "\n",
                "            break\n",
                "\n",
                "print('Complete')\n",
                "env.render()\n",
                "env.close()\n",
                "plt.ioff()\n",
                "plt.show()"
            ]
        }
    ]
}
